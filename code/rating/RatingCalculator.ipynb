{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read raw ratings and biased/ unbiased data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '../../data/results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Raw Results to do Rating Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should be getting them from bias spec file\n",
    "biastypes = ['b', 'u']\n",
    "u_vals = [ \n",
    "         ['.5', '.5']\n",
    "       ] \n",
    "b_vals = [ ['.1', '.9'] ,\n",
    "         ['.9', '.1'],\n",
    "       ] \n",
    "\n",
    "# We should get this from a config file\n",
    "wp = ['p1','p2', 'p3', 'p4', 'p5']\n",
    "\n",
    "# List of models we are handling\n",
    "models = ['textblob', 'vader', 'cnn', 'lstm', 'gru']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a wordpair, get files\n",
    "def getFileNamesForWordPairs(w):\n",
    "\n",
    "    files = []\n",
    "    filename = ''\n",
    "\n",
    "    for bt in biastypes:\n",
    "        if (bt == 'u'):\n",
    "            for uv in u_vals:\n",
    "                filename = 'result_' + w + \"_\" + bt + '_' + uv[0] + '_' + uv[1] + '.csv'\n",
    "                #print (filename)\n",
    "                files.append(filename)\n",
    "                \n",
    "        if (bt == 'b'):\n",
    "            for bv in b_vals:\n",
    "                filename = 'result_' + w + \"_\" + bt + '_' + bv[0] + '_' + bv[1] + '.csv'\n",
    "                #print (filename)\n",
    "                files.append(filename)\n",
    "                \n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'p1': ['result_p1_b_.1_.9.csv', 'result_p1_b_.9_.1.csv', 'result_p1_u_.5_.5.csv'], 'p2': ['result_p2_b_.1_.9.csv', 'result_p2_b_.9_.1.csv', 'result_p2_u_.5_.5.csv'], 'p3': ['result_p3_b_.1_.9.csv', 'result_p3_b_.9_.1.csv', 'result_p3_u_.5_.5.csv'], 'p4': ['result_p4_b_.1_.9.csv', 'result_p4_b_.9_.1.csv', 'result_p4_u_.5_.5.csv'], 'p5': ['result_p5_b_.1_.9.csv', 'result_p5_b_.9_.1.csv', 'result_p5_u_.5_.5.csv']}\n"
     ]
    }
   ],
   "source": [
    "# Make dictionary of word phrases and files\n",
    "wp_files_dict = {}\n",
    "for w in wp:\n",
    "    wp_files_dict[w] = getFileNamesForWordPairs(w)\n",
    "print (wp_files_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used while development, not needed now\n",
    "def process_sentiment_freqcount_file_orig(f):\n",
    "    file = datadir + modeldir + f\n",
    "    data = pd.read_csv(file).drop(['Unnamed: 0', 'Subjectivity'],axis=1)\n",
    "    print (data['Gender'].value_counts())\n",
    "    print (data['Sentiment'].value_counts())\n",
    "    print (data.groupby('Gender').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentiment stats\n",
    "def process_sentiment_freqcount_file(model, f):\n",
    "    file = datadir + model + '/' + f\n",
    "    data = pd.read_csv(file).drop(['Unnamed: 0'],axis=1)\n",
    "    #data = pd.read_csv(file).drop(['Unnamed: 0', 'Subjectivity'],axis=1)\n",
    "    \n",
    "    # -- For type\n",
    "    result_type = 'u'\n",
    "    if '_b_' in f:\n",
    "        result_type = 'b'\n",
    "    \n",
    "    # -- For male/ female freq\n",
    "    malecount = 0\n",
    "    femalecount = 0\n",
    "    for val, count in data.Gender.value_counts().iteritems():\n",
    "        if (val == 'male'):\n",
    "            malecount = count\n",
    "        else:\n",
    "            femalecount = count\n",
    "        if (val == 'female'):\n",
    "            femalecount = count\n",
    "        else:\n",
    "            malecount = count    \n",
    "        # print ('value', val, 'was found', count, 'times')\n",
    "    #print (f\"Counts: male = {malecount}, female = {femalecount}\")\n",
    "    ## print (data['Gender'].value_counts())\n",
    "\n",
    "    \n",
    "\n",
    "    # -- For gender-based sentiment stats\n",
    "    maleavg = 0\n",
    "    femaleavg = 0\n",
    "    for name, group in data.groupby('Gender'):\n",
    "        avg = float (group.mean())\n",
    "        if (name == 'male'):\n",
    "            maleavg = avg\n",
    "        else:\n",
    "            femaleavg = avg\n",
    "        if (name == 'female'):\n",
    "            femaleavg = avg\n",
    "        else:\n",
    "            maleavg = avg  \n",
    "    #print (f\"Sentiments avg: male = {maleavg}, female = {femaleavg}\")\n",
    "        ## print (f\"Counts: name = {name}, mean = {group.mean()}\")\n",
    "        \n",
    "    \n",
    "    #print (data.groupby('Gender').mean())\n",
    "    \n",
    "        \n",
    "    # -- For sentiment freq\n",
    "    sentiment_freq = []\n",
    "    for val, count in data.Sentiment.value_counts().iteritems():   \n",
    "        # print ('value', val, 'was found', count, 'times')\n",
    "        sentiment_freq.append([val, count])\n",
    "    \n",
    "    #print (sentiment_freq)\n",
    "    ##print (data['Sentiment'].value_counts())\n",
    "    \n",
    "    # -- For returning raw sentiment scores as a lost\n",
    "    senti_scores = data['Sentiment'].tolist()\n",
    "    \n",
    "    return [result_type, malecount, femalecount, maleavg, femaleavg, sentiment_freq, senti_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recording Stats in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For given data type, do calculation of raw stats and rating\n",
    "\n",
    "def calculate_ratings_stats(data_type):\n",
    "    \n",
    "    # Create a dataframe with rating. We can later save it as .csv\n",
    "    stats = pd.DataFrame(columns = ['Model', 'WordPairs', 'Filename', 'Type', 'MaleCount', 'FemaleCount', 'MaleAvSentim', 'FemaleAvSentim', 'SentiFreq', 'RawSentiScores']) \n",
    "\n",
    "    # For each model, for each word pair, for all the files\n",
    "    t0 = []\n",
    "    # models = ['textblob', 'vader', 'cnn']  # Using global\n",
    "    for m in models: # For each sentiment model\n",
    "        t1 = t0.copy()\n",
    "        t1.append(m)\n",
    "        for w in wp: # For each word phrase\n",
    "            t2 = t1.copy()\n",
    "            t2.append(w)\n",
    "            fl = wp_files_dict[w]\n",
    "            for f in fl: # For each file\n",
    "                t3 = t2.copy()\n",
    "                t3.append(f)\n",
    "                t3 = t3 + process_sentiment_freqcount_file(m + \"/\" + data_type, f)\n",
    "                # Add the row now\n",
    "                arow = t3\n",
    "                # print (arow)\n",
    "                stats.loc[len(stats)] = arow\n",
    "                \n",
    "    # print (stats)\n",
    "    \n",
    "    # Store stats dataframe\n",
    "    store_result_dataframe (stats, data_type + \"-\" + \"stats-analysis.csv\")\n",
    "    \n",
    "    # Return stats\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Rating File to Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/results/rating folder already exists.\n"
     ]
    }
   ],
   "source": [
    "ratingdir = '../../data/results/rating'\n",
    "\n",
    "'''Check if directory exists, if not, create it'''\n",
    "import os\n",
    "\n",
    "# You should change 'test' to your preferred folder.\n",
    "check_folder = os.path.isdir(ratingdir)\n",
    "\n",
    "# If folder doesn't exist, then create it.\n",
    "if not check_folder:\n",
    "    os.makedirs(ratingdir)\n",
    "    print(\"created folder : \", ratingdir)\n",
    "\n",
    "else:\n",
    "    print(ratingdir, \"folder already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe df in rating dir in file called filename\n",
    "def store_result_dataframe (df, filename):\n",
    "    outfile = ratingdir + '/' + filename\n",
    "    df.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare via KL-divergence\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare list a and b using entropy. The difference has to be more than\n",
    "# threshold for the answer to be different\n",
    "def are_distribs_different(a, b):\n",
    "    threshold  = 0.3\n",
    "    \n",
    "    v = entropy(a, b)\n",
    "    \n",
    "    if v > threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare unbiased with biased in a 2-step procedure\n",
    "def calculate_rating(subset):\n",
    "    \n",
    "    bias_results = []\n",
    "    unbias_results = []\n",
    "    \n",
    "    # Keep track of unbiased and biased raw results in lists\n",
    "    for index, row in subset.iterrows():\n",
    "        if row['Type'] == 'b':\n",
    "            bias_results.append(row)\n",
    "        else:\n",
    "            unbias_results.append(row)\n",
    "       \n",
    "    # Step 1: compare unbias with biased\n",
    "    for u in unbias_results:\n",
    "        for b in bias_results:\n",
    "            \n",
    "            v1 = [ u['MaleAvSentim'], u['FemaleAvSentim'] ]\n",
    "            v2 = [ b['MaleAvSentim'], b['FemaleAvSentim']]\n",
    "    \n",
    "            step1_result = are_distribs_different(v1, v2)\n",
    "        \n",
    "            # If unbias output is different than even one bias spec, \n",
    "            # call result as biased\n",
    "            \n",
    "            if(step1_result):\n",
    "                # print ('Step 1: ', v1, v2, result)\n",
    "                return 'BS'\n",
    "            \n",
    "    # Step 2: Step 1 has lead to unbiased results. Now compare when input\n",
    "    #         is biased with output of unbiased spec\n",
    "            \n",
    "    for b in bias_results:\n",
    "        for u in unbias_results:\n",
    "          \n",
    "            v1 = [ b['MaleAvSentim'], b['FemaleAvSentim']]\n",
    "            v2 = [ u['MaleAvSentim'], u['FemaleAvSentim'] ]\n",
    "\n",
    "    \n",
    "            step2_result = are_distribs_different(v1, v2)\n",
    "        \n",
    "            if(step2_result):\n",
    "                # print ('Step 2: ', v1, v2, result)\n",
    "                return 'UCS'\n",
    "            \n",
    "    return 'DSBS'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Do the calculation of ratings from stats calculated\n",
    "\n",
    "def calculate_final_rating (data_type, stats):\n",
    "    \n",
    "    # Store rating as a dataframe\n",
    "    rating = pd.DataFrame(columns = ['Model', 'WordPairs', 'Rating']) \n",
    "\n",
    "    # For each model, for each word pair, get a slice of data\n",
    "\n",
    "    # models = ['textblob', 'vader', 'cnn']  # Using global\n",
    "    for m in models: # For each sentiment model\n",
    "        # Track the worst rating so that we can assign it at the end\n",
    "        worst_rating = 'UCS'\n",
    "        for w in wp: # For each word phrase\n",
    "        \n",
    "            # Get slice of data frame\n",
    "            mwp_subset = stats[(stats['Model'] == m) & (stats['WordPairs'] == w)]\n",
    "        \n",
    "            # Now call for rating\n",
    "            rating_value = calculate_rating(mwp_subset)\n",
    "        \n",
    "            # Track worst rating across word phrases\n",
    "            if ((rating_value == 'DSBS') & (worst_rating == 'UCS')):\n",
    "                worst_rating = 'DSBS'\n",
    "            if ((rating_value == 'BS') & (worst_rating == 'UCS')):\n",
    "                worst_rating = 'BS'\n",
    "            if ((rating_value == 'BS') & (worst_rating == 'DSBS')):\n",
    "                worst_rating = 'BS'\n",
    "        \n",
    "            # Record the rating for word phrase\n",
    "            rating.loc[len(rating)] = [m, w, rating_value]\n",
    "            \n",
    "        # Overall rating\n",
    "        rating.loc[len(rating)] = [m, 'Overall', worst_rating]\n",
    "        \n",
    "    # print (rating)\n",
    "    \n",
    "    # Store rating in file\n",
    "    store_result_dataframe(rating, data_type + \"-rating.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does Actual Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main routine for each data_type\n",
    "\n",
    "data_types = ['nonames', 'withnames']\n",
    "for d in data_types:\n",
    "    stats = calculate_ratings_stats(d)\n",
    "    calculate_final_rating(d, stats)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
